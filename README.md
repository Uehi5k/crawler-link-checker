# Crawler Link Checker

This tool is to provide a convenient way for the users to check if there are any broken links on a website. There are other third party tools available that can behave the same, however for personal needs, I decide to create a free version of the tool, and share for anyone who wants to use.

What the tool provides:
- Given an URL, it will check
  - **img tag** - looking at the **src** and **srcset** attributes for the list of URLs.
  - **meta tag** - looking for **content** attribute and filter those matching the a format of an URL.
  - **link tag** - looking for **href** attribute.
  - **script tag** - looking for **src** attribute.
  - **a tag with .docx or .pdf** - specifically looking for Word doc or PDF files in the a tag.
  - **a tag** - looking for **href** attribute.
- Capability to craw a single page, or recursive crawling until all pages of the same domain have been crawled.
- Some websites using bot protection, and prevent crawler accessing the webpage. To solve this problem, when the crawler has a failed status for one of the url, it will then be pushed to the **failed-route**, and in this route, it will use a fallback solution using [gotScraping](https://crawlee.dev/docs/examples/crawl-single-url) to check for the URL, and based on the status of the response, it will then be logged correctly. To understand more about **gotScarping**, follow this article [Got Scraping](https://crawlee.dev/docs/guides/got-scraping).
- Save data using timestamp based method. If you scrape a website, the domain of the url will be used in combination with the timestamp using `new Date().getTime()`. A CSV and a JSON files are provided when crawler finishes.
- Simple UI to crawl a single page, howeve you can choose to opt-in the recursive option to crawl all links within the same domain.

The tech stack includes:
- [Crawlee](https://crawlee.dev/) - PlaywrightCrawler is used to crawl.
- [ElysiaJS](https://elysiajs.com/) - Simple UI and endpoints for calling a crawler, mostly for demonstration purposes.

# Pre-requisite

Node version 20 or above. Version used to build this tool - **20.11.1**.

# Run the application

Run `npm install` and then `npm run start`.

# Data examples
Please find the CSV and JSON files under **examples** folder for reference.

# To do list

1. Demo
2. UI enhancement to input a URL and configurations.
3. Dockerfile
4. Fix production build
5. Singleton Crawler

# Troubleshooting

Some of the errors I have encountered when crawling website

**Block resource results in 403, happens for redirection URL or bot blocking detection tool - fixed using gotScraping**
```
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<HTML><HEAD><META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<TITLE>ERROR: The request could not be satisfied</TITLE>
</HEAD><BODY>
<H1>403 ERROR</H1>
<H2>The request could not be satisfied.</H2>
<HR noshade size="1px">
Request blocked.
We can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later, or contact the app or website owner.
<BR clear="all">
If you provide content to customers through CloudFront, you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation.
<BR clear="all">
<HR noshade size="1px">
<PRE>
Generated by cloudfront (CloudFront)
Request ID: 4kVsbQTqEZydcdRdQUZXr4IRUl800kXGryj8XJZduvHQ3C_jx9DaBQ==
</PRE>
<ADDRESS>
</ADDRESS>
</BODY></HTML>
```

**Timeout issue stops crawler - rare**
```
node:internal/process/esm_loader:40
      internalBinding('errors').triggerUncaughtException(
                                ^

RequestError: connect ETIMEDOUT 103.1.193.204:443
    at Request._beforeError (file:///D:/Documents/Job%20Documents/Projects/Freelance%20Projects/Personal%20Project/crawler-link-checker/node_modules/got/dist/source/core/index.js:189:21)
    at Request.flush (file:///D:/Documents/Job%20Documents/Projects/Freelance%20Projects/Personal%20Project/crawler-link-checker/node_modules/got/dist/source/core/index.js:178:18)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at __node_internal_captureLargerStackTrace (node:internal/errors:496:5)
    ... 2 lines matching cause stack trace ...
    at TCPConnectWrap.callbackTrampoline (node:internal/async_hooks:128:17) 

      [cause]: Error: connect ETIMEDOUT 103.1.193.204:443
      at __node_internal_captureLargerStackTrace (node:internal/errors:496:5)
      at __node_internal_exceptionWithHostPort (node:internal/errors:671:12)
      at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)
      at TCPConnectWrap.callbackTrampoline (node:internal/async_hooks:128:17) {
    errno: -4039,
    code: 'ETIMEDOUT',
    syscall: 'connect',
    address: '103.1.193.204',
    port: 443
  }
```